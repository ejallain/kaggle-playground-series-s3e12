{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nb_utils\n",
    "proj_dir = nb_utils.proj_path_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(proj_dir, 'data', 'raw', 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(proj_dir, 'data', 'raw', 'test.csv'))\n",
    "feature_cols = [col for col in test_df.columns if col != 'id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(train_df[feature_cols])\n",
    "\n",
    "y_train = np.array(train_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.73403157,  0.36562268, -0.8897276 , -0.88438794, -1.13486936,\n",
       "        -0.82913603],\n",
       "       [ 1.06590369, -0.86589692,  0.21952069,  0.28805926,  0.84638411,\n",
       "         0.02033832],\n",
       "       [-1.33400999,  0.27208954, -1.19690404,  0.40796863, -0.8780402 ,\n",
       "         1.53258937],\n",
       "       ...,\n",
       "       [ 0.01594146,  0.50592238, -0.83853152,  0.10153357, -0.06352489,\n",
       "         1.109408  ],\n",
       "       [-1.48400459,  1.81538625, -1.39315566, -1.17749974, -1.49443017,\n",
       "        -0.9598244 ],\n",
       "       [-1.03402078,  0.27208954, -1.22676842, -1.53722786, -0.8780402 ,\n",
       "        -1.1963081 ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add noise to X data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.dae import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train, columns=feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_X = add_noise(X_train_df, p=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gravity</th>\n",
       "      <th>ph</th>\n",
       "      <th>osmo</th>\n",
       "      <th>cond</th>\n",
       "      <th>urea</th>\n",
       "      <th>calc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.734032</td>\n",
       "      <td>-0.585298</td>\n",
       "      <td>-0.889728</td>\n",
       "      <td>0.781020</td>\n",
       "      <td>-1.134869</td>\n",
       "      <td>-0.041894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.065904</td>\n",
       "      <td>-0.865897</td>\n",
       "      <td>0.219521</td>\n",
       "      <td>0.288059</td>\n",
       "      <td>0.237332</td>\n",
       "      <td>-1.022057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.334010</td>\n",
       "      <td>0.272090</td>\n",
       "      <td>-1.196904</td>\n",
       "      <td>0.407969</td>\n",
       "      <td>0.758328</td>\n",
       "      <td>1.532589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.315931</td>\n",
       "      <td>-1.629751</td>\n",
       "      <td>-0.893994</td>\n",
       "      <td>-0.084992</td>\n",
       "      <td>0.875736</td>\n",
       "      <td>-0.203699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.465925</td>\n",
       "      <td>-0.663242</td>\n",
       "      <td>0.949065</td>\n",
       "      <td>-1.230793</td>\n",
       "      <td>-1.120193</td>\n",
       "      <td>-0.592652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>0.915909</td>\n",
       "      <td>-1.162085</td>\n",
       "      <td>-1.316362</td>\n",
       "      <td>-0.005053</td>\n",
       "      <td>-1.494430</td>\n",
       "      <td>-0.804243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>0.915909</td>\n",
       "      <td>-0.756775</td>\n",
       "      <td>-0.318038</td>\n",
       "      <td>-0.071669</td>\n",
       "      <td>-0.401072</td>\n",
       "      <td>0.116799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>-0.134053</td>\n",
       "      <td>0.505922</td>\n",
       "      <td>-0.838532</td>\n",
       "      <td>0.101534</td>\n",
       "      <td>-0.063525</td>\n",
       "      <td>1.358338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>-1.484005</td>\n",
       "      <td>-0.663242</td>\n",
       "      <td>-1.393156</td>\n",
       "      <td>0.567848</td>\n",
       "      <td>-1.494430</td>\n",
       "      <td>-0.959824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>-1.034021</td>\n",
       "      <td>-0.522942</td>\n",
       "      <td>-1.226768</td>\n",
       "      <td>-1.537228</td>\n",
       "      <td>-0.878040</td>\n",
       "      <td>-1.196308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>414 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      gravity        ph      osmo      cond      urea      calc\n",
       "0   -0.734032 -0.585298 -0.889728  0.781020 -1.134869 -0.041894\n",
       "1    1.065904 -0.865897  0.219521  0.288059  0.237332 -1.022057\n",
       "2   -1.334010  0.272090 -1.196904  0.407969  0.758328  1.532589\n",
       "3    0.315931 -1.629751 -0.893994 -0.084992  0.875736 -0.203699\n",
       "4    0.465925 -0.663242  0.949065 -1.230793 -1.120193 -0.592652\n",
       "..        ...       ...       ...       ...       ...       ...\n",
       "409  0.915909 -1.162085 -1.316362 -0.005053 -1.494430 -0.804243\n",
       "410  0.915909 -0.756775 -0.318038 -0.071669 -0.401072  0.116799\n",
       "411 -0.134053  0.505922 -0.838532  0.101534 -0.063525  1.358338\n",
       "412 -1.484005 -0.663242 -1.393156  0.567848 -1.494430 -0.959824\n",
       "413 -1.034021 -0.522942 -1.226768 -1.537228 -0.878040 -1.196308\n",
       "\n",
       "[414 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gravity</th>\n",
       "      <th>ph</th>\n",
       "      <th>osmo</th>\n",
       "      <th>cond</th>\n",
       "      <th>urea</th>\n",
       "      <th>calc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.013</td>\n",
       "      <td>6.19</td>\n",
       "      <td>443</td>\n",
       "      <td>14.8</td>\n",
       "      <td>124</td>\n",
       "      <td>1.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.025</td>\n",
       "      <td>5.40</td>\n",
       "      <td>703</td>\n",
       "      <td>23.6</td>\n",
       "      <td>394</td>\n",
       "      <td>4.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.009</td>\n",
       "      <td>6.13</td>\n",
       "      <td>371</td>\n",
       "      <td>24.5</td>\n",
       "      <td>159</td>\n",
       "      <td>9.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.021</td>\n",
       "      <td>4.91</td>\n",
       "      <td>442</td>\n",
       "      <td>20.8</td>\n",
       "      <td>398</td>\n",
       "      <td>6.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.021</td>\n",
       "      <td>5.53</td>\n",
       "      <td>874</td>\n",
       "      <td>17.8</td>\n",
       "      <td>385</td>\n",
       "      <td>2.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>1.011</td>\n",
       "      <td>5.21</td>\n",
       "      <td>527</td>\n",
       "      <td>21.4</td>\n",
       "      <td>75</td>\n",
       "      <td>1.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>1.024</td>\n",
       "      <td>5.53</td>\n",
       "      <td>577</td>\n",
       "      <td>19.7</td>\n",
       "      <td>224</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1.018</td>\n",
       "      <td>6.28</td>\n",
       "      <td>455</td>\n",
       "      <td>22.2</td>\n",
       "      <td>270</td>\n",
       "      <td>7.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1.008</td>\n",
       "      <td>7.12</td>\n",
       "      <td>325</td>\n",
       "      <td>12.6</td>\n",
       "      <td>75</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1.011</td>\n",
       "      <td>6.13</td>\n",
       "      <td>364</td>\n",
       "      <td>9.9</td>\n",
       "      <td>159</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>414 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gravity    ph  osmo  cond  urea  calc\n",
       "0      1.013  6.19   443  14.8   124  1.45\n",
       "1      1.025  5.40   703  23.6   394  4.18\n",
       "2      1.009  6.13   371  24.5   159  9.04\n",
       "3      1.021  4.91   442  20.8   398  6.63\n",
       "4      1.021  5.53   874  17.8   385  2.21\n",
       "..       ...   ...   ...   ...   ...   ...\n",
       "409    1.011  5.21   527  21.4    75  1.53\n",
       "410    1.024  5.53   577  19.7   224  0.77\n",
       "411    1.018  6.28   455  22.2   270  7.68\n",
       "412    1.008  7.12   325  12.6    75  1.03\n",
       "413    1.011  6.13   364   9.9   159  0.27\n",
       "\n",
       "[414 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=noisy_X.shape[1:]),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dense(6)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.5856\n",
      "Epoch 2/2001\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.4043\n",
      "Epoch 3/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3568\n",
      "Epoch 4/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3313\n",
      "Epoch 5/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3069\n",
      "Epoch 6/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2955\n",
      "Epoch 7/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2806\n",
      "Epoch 8/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2677\n",
      "Epoch 9/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2537\n",
      "Epoch 10/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2413\n",
      "Epoch 11/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2305\n",
      "Epoch 12/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2223\n",
      "Epoch 13/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2163\n",
      "Epoch 14/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2116\n",
      "Epoch 15/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1998\n",
      "Epoch 16/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1939\n",
      "Epoch 17/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1825\n",
      "Epoch 18/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1803\n",
      "Epoch 19/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1729\n",
      "Epoch 20/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1619\n",
      "Epoch 21/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1537\n",
      "Epoch 22/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1485\n",
      "Epoch 23/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1420\n",
      "Epoch 24/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1356\n",
      "Epoch 25/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1302\n",
      "Epoch 26/2001\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1255\n",
      "Epoch 27/2001\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1165\n",
      "Epoch 28/2001\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1158\n",
      "Epoch 29/2001\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1126\n",
      "Epoch 30/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1014\n",
      "Epoch 31/2001\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0957\n",
      "Epoch 32/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0929\n",
      "Epoch 33/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0878\n",
      "Epoch 34/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0824\n",
      "Epoch 35/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0789\n",
      "Epoch 36/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0724\n",
      "Epoch 37/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0707\n",
      "Epoch 38/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0766\n",
      "Epoch 39/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0676\n",
      "Epoch 40/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0651\n",
      "Epoch 41/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0574\n",
      "Epoch 42/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0509\n",
      "Epoch 43/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0474\n",
      "Epoch 44/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0492\n",
      "Epoch 45/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0472\n",
      "Epoch 46/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0499\n",
      "Epoch 47/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0512\n",
      "Epoch 48/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0478\n",
      "Epoch 49/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0457\n",
      "Epoch 50/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0431\n",
      "Epoch 51/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0367\n",
      "Epoch 52/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0377\n",
      "Epoch 53/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0358\n",
      "Epoch 54/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0316\n",
      "Epoch 55/2001\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0293\n",
      "Epoch 56/2001\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0293\n",
      "Epoch 57/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0305\n",
      "Epoch 58/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0300\n",
      "Epoch 59/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0292\n",
      "Epoch 60/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0317\n",
      "Epoch 61/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0328\n",
      "Epoch 62/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0343\n",
      "Epoch 63/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0292\n",
      "Epoch 64/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0258\n",
      "Epoch 65/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0256\n",
      "Epoch 66/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0257\n",
      "Epoch 67/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0247\n",
      "Epoch 68/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0234\n",
      "Epoch 69/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0216\n",
      "Epoch 70/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0215\n",
      "Epoch 71/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0216\n",
      "Epoch 72/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0217\n",
      "Epoch 73/2001\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0203\n",
      "Epoch 74/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0205\n",
      "Epoch 75/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0197\n",
      "Epoch 76/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0194\n",
      "Epoch 77/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0222\n",
      "Epoch 78/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0222\n",
      "Epoch 79/2001\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0181\n",
      "Epoch 80/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0169\n",
      "Epoch 81/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0168\n",
      "Epoch 82/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0154\n",
      "Epoch 83/2001\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0146\n",
      "Epoch 84/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0149\n",
      "Epoch 85/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0139\n",
      "Epoch 86/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0140\n",
      "Epoch 87/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0138\n",
      "Epoch 88/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0155\n",
      "Epoch 89/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0162\n",
      "Epoch 90/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0159\n",
      "Epoch 91/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0149\n",
      "Epoch 92/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0131\n",
      "Epoch 93/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0118\n",
      "Epoch 94/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0099\n",
      "Epoch 95/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0112\n",
      "Epoch 96/2001\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0101\n",
      "Epoch 97/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0091\n",
      "Epoch 98/2001\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0091\n",
      "Epoch 99/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0089\n",
      "Epoch 100/2001\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0083\n",
      "Epoch 101/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0077\n",
      "Epoch 102/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0073\n",
      "Epoch 103/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0084\n",
      "Epoch 104/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0084\n",
      "Epoch 105/2001\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0105\n",
      "Epoch 106/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0095\n",
      "Epoch 107/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0110\n",
      "Epoch 108/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0144\n",
      "Epoch 109/2001\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0155\n",
      "Epoch 110/2001\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0148\n",
      "Epoch 111/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0149\n",
      "Epoch 112/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0145\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(noisy_X, X_train_df, epochs=2001,\n",
    "                    callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(proj_dir, 'models', 'dae_model_swap_noise_30.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "features_df = extract_features(model, X_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1490</th>\n",
       "      <th>1491</th>\n",
       "      <th>1492</th>\n",
       "      <th>1493</th>\n",
       "      <th>1494</th>\n",
       "      <th>1495</th>\n",
       "      <th>1496</th>\n",
       "      <th>1497</th>\n",
       "      <th>1498</th>\n",
       "      <th>1499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.026056</td>\n",
       "      <td>0.001995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.205874</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169563</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133441</td>\n",
       "      <td>0.280636</td>\n",
       "      <td>0.031604</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.088951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065648</td>\n",
       "      <td>0.193719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139392</td>\n",
       "      <td>0.225901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156399</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064072</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054823</td>\n",
       "      <td>0.592438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137512</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.118008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046430</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009167</td>\n",
       "      <td>0.073100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.220394</td>\n",
       "      <td>0.285846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004463</td>\n",
       "      <td>0.033996</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051976</td>\n",
       "      <td>0.420820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.121445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297292</td>\n",
       "      <td>0.392665</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144395</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.162063</td>\n",
       "      <td>0.075195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123515</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089504</td>\n",
       "      <td>0.047747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182555</td>\n",
       "      <td>0.003599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123215</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066252</td>\n",
       "      <td>0.281874</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156932</td>\n",
       "      <td>0.540897</td>\n",
       "      <td>0.221749</td>\n",
       "      <td>0.344451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>0.094706</td>\n",
       "      <td>0.004296</td>\n",
       "      <td>0.020253</td>\n",
       "      <td>0.012416</td>\n",
       "      <td>0.208547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043502</td>\n",
       "      <td>0.168264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.221199</td>\n",
       "      <td>0.388686</td>\n",
       "      <td>0.114813</td>\n",
       "      <td>0.069366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>0.018715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.226792</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139316</td>\n",
       "      <td>0.108907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014743</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142996</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>0.053399</td>\n",
       "      <td>0.118599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.202642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.402476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.239560</td>\n",
       "      <td>0.111723</td>\n",
       "      <td>0.026075</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0.158253</td>\n",
       "      <td>0.022868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030468</td>\n",
       "      <td>0.095591</td>\n",
       "      <td>0.013458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.323431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.329876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176225</td>\n",
       "      <td>0.466309</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>414 rows Ã— 1500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6     \\\n",
       "0    0.026056  0.001995  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1    0.088951  0.000000  0.000000  0.065648  0.193719  0.000000  0.041704   \n",
       "2    0.000000  0.000000  0.295833  0.000000  0.000000  0.510566  0.000000   \n",
       "3    0.118008  0.000000  0.046430  0.000000  0.009167  0.073100  0.000000   \n",
       "4    0.121445  0.000000  0.000000  0.297292  0.392665  0.000000  0.144395   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "409  0.000000  0.000000  0.182555  0.003599  0.000000  0.000000  0.000000   \n",
       "410  0.094706  0.004296  0.020253  0.012416  0.208547  0.000000  0.098938   \n",
       "411  0.018715  0.000000  0.127083  0.000000  0.000000  0.226792  0.000000   \n",
       "412  0.053399  0.118599  0.000000  0.000000  0.000000  0.202642  0.000000   \n",
       "413  0.158253  0.022868  0.000000  0.030468  0.095591  0.013458  0.000000   \n",
       "\n",
       "         7         8         9     ...      1490      1491      1492  \\\n",
       "0    0.205874  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "1    0.000000  0.139392  0.225901  ...  0.075326  0.000000  0.156399   \n",
       "2    0.095750  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3    0.000000  0.220394  0.285846  ...  0.004463  0.033996  0.000000   \n",
       "4    0.000000  0.162063  0.075195  ...  0.000000  0.000000  0.123515   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "409  0.123215  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "410  0.000000  0.043502  0.168264  ...  0.012065  0.000000  0.000000   \n",
       "411  0.000000  0.000000  0.000000  ...  0.139316  0.108907  0.000000   \n",
       "412  0.402476  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "413  0.323431  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "         1493      1494      1495      1496      1497      1498      1499  \n",
       "0    0.000000  0.169563  0.000000  0.133441  0.280636  0.031604  0.000000  \n",
       "1    0.000000  0.000000  0.000000  0.000000  0.000000  0.064072  0.000000  \n",
       "2    0.297476  0.000000  0.054823  0.592438  0.000000  0.137512  0.000000  \n",
       "3    0.000000  0.051976  0.420820  0.000000  0.000000  0.000000  0.000000  \n",
       "4    0.000000  0.000000  0.000000  0.000000  0.000000  0.089504  0.047747  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "409  0.066252  0.281874  0.000000  0.156932  0.540897  0.221749  0.344451  \n",
       "410  0.027539  0.000000  0.000000  0.221199  0.388686  0.114813  0.069366  \n",
       "411  0.014743  0.000000  0.000000  0.142996  0.000000  0.000000  0.000000  \n",
       "412  0.000000  0.194837  0.000000  0.239560  0.111723  0.026075  0.000000  \n",
       "413  0.000000  0.329876  0.000000  0.176225  0.466309  0.000000  0.000000  \n",
       "\n",
       "[414 rows x 1500 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test logistic model fit using extracted dae features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(features_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_model.predict(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode test data and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(test_df[feature_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df = pd.DataFrame(X_test, columns=feature_cols)\n",
    "noisy_X_test = add_noise(X_test_df, p=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.6599\n",
      "Epoch 2/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.3591\n",
      "Epoch 3/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.2872\n",
      "Epoch 4/2001\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.2343\n",
      "Epoch 5/2001\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.2004\n",
      "Epoch 6/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.1693\n",
      "Epoch 7/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.1482\n",
      "Epoch 8/2001\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.1331\n",
      "Epoch 9/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.1186\n",
      "Epoch 10/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.1048\n",
      "Epoch 11/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0957\n",
      "Epoch 12/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0825\n",
      "Epoch 13/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0775\n",
      "Epoch 14/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0670\n",
      "Epoch 15/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0628\n",
      "Epoch 16/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0543\n",
      "Epoch 17/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0487\n",
      "Epoch 18/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0442\n",
      "Epoch 19/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0423\n",
      "Epoch 20/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0393\n",
      "Epoch 21/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0357\n",
      "Epoch 22/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0331\n",
      "Epoch 23/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0307\n",
      "Epoch 24/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0288\n",
      "Epoch 25/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0253\n",
      "Epoch 26/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0228\n",
      "Epoch 27/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0212\n",
      "Epoch 28/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0206\n",
      "Epoch 29/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0195\n",
      "Epoch 30/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0177\n",
      "Epoch 31/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0159\n",
      "Epoch 32/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0145\n",
      "Epoch 33/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0134\n",
      "Epoch 34/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0117\n",
      "Epoch 35/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0121\n",
      "Epoch 36/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0124\n",
      "Epoch 37/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0133\n",
      "Epoch 38/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0138\n",
      "Epoch 39/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0129\n",
      "Epoch 40/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0123\n",
      "Epoch 41/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0115\n",
      "Epoch 42/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0107\n",
      "Epoch 43/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0096\n",
      "Epoch 44/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0096\n",
      "Epoch 45/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0091\n",
      "Epoch 46/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0090\n",
      "Epoch 47/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0088\n",
      "Epoch 48/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0084\n",
      "Epoch 49/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0081\n",
      "Epoch 50/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0075\n",
      "Epoch 51/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0086\n",
      "Epoch 52/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0077\n",
      "Epoch 53/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0075\n",
      "Epoch 54/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0069\n",
      "Epoch 55/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0069\n",
      "Epoch 56/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0064\n",
      "Epoch 57/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0056\n",
      "Epoch 58/2001\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0050\n",
      "Epoch 59/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0044\n",
      "Epoch 60/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0048\n",
      "Epoch 61/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0050\n",
      "Epoch 62/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0044\n",
      "Epoch 63/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0042\n",
      "Epoch 64/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0038\n",
      "Epoch 65/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0033\n",
      "Epoch 66/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0037\n",
      "Epoch 67/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0039\n",
      "Epoch 68/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0036\n",
      "Epoch 69/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0061\n",
      "Epoch 70/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0044\n",
      "Epoch 71/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0032\n",
      "Epoch 72/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0035\n",
      "Epoch 73/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0032\n",
      "Epoch 74/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0030\n",
      "Epoch 75/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0034\n",
      "Epoch 76/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0029\n",
      "Epoch 77/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0027\n",
      "Epoch 78/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0028\n",
      "Epoch 79/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0028\n",
      "Epoch 80/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0023\n",
      "Epoch 81/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0022\n",
      "Epoch 82/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0020\n",
      "Epoch 83/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0018\n",
      "Epoch 84/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0015\n",
      "Epoch 85/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0016\n",
      "Epoch 86/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0014\n",
      "Epoch 87/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0012\n",
      "Epoch 88/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0012\n",
      "Epoch 89/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0012\n",
      "Epoch 90/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0012\n",
      "Epoch 91/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0012\n",
      "Epoch 92/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0011\n",
      "Epoch 93/2001\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0010\n",
      "Epoch 94/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 8.2779e-04\n",
      "Epoch 95/2001\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 8.2063e-04\n",
      "Epoch 96/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 6.5041e-04\n",
      "Epoch 97/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 6.3287e-04\n",
      "Epoch 98/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 5.9061e-04\n",
      "Epoch 99/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 5.9724e-04\n",
      "Epoch 100/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 6.0040e-04\n",
      "Epoch 101/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 5.7687e-04\n",
      "Epoch 102/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 6.5871e-04\n",
      "Epoch 103/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 6.9540e-04\n",
      "Epoch 104/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 9.6163e-04\n",
      "Epoch 105/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0013\n",
      "Epoch 106/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0013\n",
      "Epoch 107/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0013\n",
      "Epoch 108/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0014\n",
      "Epoch 109/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0015\n",
      "Epoch 110/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0016\n",
      "Epoch 111/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0017\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(noisy_X_test, X_test_df, epochs=2001,\n",
    "                    callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "test_features_df = extract_features(model, X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_model.predict(test_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = test_df.copy()\n",
    "sub_df['target'] = y_pred\n",
    "sub_df = sub_df[['id', 'target']]\n",
    "sub_df.to_csv(os.path.join(proj_dir, 'data', 'submissions', 'dae_sn30_model_submission_1.csv'), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitting the above gives me a score of 0.75333 ... even worse than the lr model alone.  Obviously need to tune at least the swap_noise probability.\n",
    "\n",
    "I think it may work better if I use both the train and test X data together to train the autoencoder.  I try this below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gravity</th>\n",
       "      <th>ph</th>\n",
       "      <th>osmo</th>\n",
       "      <th>cond</th>\n",
       "      <th>urea</th>\n",
       "      <th>calc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.734032</td>\n",
       "      <td>0.365623</td>\n",
       "      <td>-0.889728</td>\n",
       "      <td>-0.884388</td>\n",
       "      <td>-1.134869</td>\n",
       "      <td>-0.829136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.065904</td>\n",
       "      <td>-0.865897</td>\n",
       "      <td>0.219521</td>\n",
       "      <td>0.288059</td>\n",
       "      <td>0.846384</td>\n",
       "      <td>0.020338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.334010</td>\n",
       "      <td>0.272090</td>\n",
       "      <td>-1.196904</td>\n",
       "      <td>0.407969</td>\n",
       "      <td>-0.878040</td>\n",
       "      <td>1.532589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.465925</td>\n",
       "      <td>-1.629751</td>\n",
       "      <td>-0.893994</td>\n",
       "      <td>-0.084992</td>\n",
       "      <td>0.875736</td>\n",
       "      <td>0.782687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.465925</td>\n",
       "      <td>-0.663242</td>\n",
       "      <td>0.949065</td>\n",
       "      <td>-0.484690</td>\n",
       "      <td>0.780342</td>\n",
       "      <td>-0.592652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>1.665882</td>\n",
       "      <td>0.490334</td>\n",
       "      <td>0.859472</td>\n",
       "      <td>-0.005053</td>\n",
       "      <td>0.626245</td>\n",
       "      <td>0.994278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>-0.884026</td>\n",
       "      <td>-0.522942</td>\n",
       "      <td>-1.030517</td>\n",
       "      <td>-0.990974</td>\n",
       "      <td>-0.613873</td>\n",
       "      <td>-0.829136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>1.965871</td>\n",
       "      <td>-0.429409</td>\n",
       "      <td>0.949065</td>\n",
       "      <td>1.007515</td>\n",
       "      <td>0.743652</td>\n",
       "      <td>0.116799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>0.165936</td>\n",
       "      <td>-0.756775</td>\n",
       "      <td>-0.463094</td>\n",
       "      <td>-0.005053</td>\n",
       "      <td>-0.797322</td>\n",
       "      <td>-0.770015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>-0.134053</td>\n",
       "      <td>0.365623</td>\n",
       "      <td>0.198189</td>\n",
       "      <td>1.007515</td>\n",
       "      <td>-0.195608</td>\n",
       "      <td>-0.181917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      gravity        ph      osmo      cond      urea      calc\n",
       "0   -0.734032  0.365623 -0.889728 -0.884388 -1.134869 -0.829136\n",
       "1    1.065904 -0.865897  0.219521  0.288059  0.846384  0.020338\n",
       "2   -1.334010  0.272090 -1.196904  0.407969 -0.878040  1.532589\n",
       "3    0.465925 -1.629751 -0.893994 -0.084992  0.875736  0.782687\n",
       "4    0.465925 -0.663242  0.949065 -0.484690  0.780342 -0.592652\n",
       "..        ...       ...       ...       ...       ...       ...\n",
       "271  1.665882  0.490334  0.859472 -0.005053  0.626245  0.994278\n",
       "272 -0.884026 -0.522942 -1.030517 -0.990974 -0.613873 -0.829136\n",
       "273  1.965871 -0.429409  0.949065  1.007515  0.743652  0.116799\n",
       "274  0.165936 -0.756775 -0.463094 -0.005053 -0.797322 -0.770015\n",
       "275 -0.134053  0.365623  0.198189  1.007515 -0.195608 -0.181917\n",
       "\n",
       "[690 rows x 6 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df = pd.concat([X_train_df, X_test_df])\n",
    "X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_X = add_noise(X_df, p=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gravity</th>\n",
       "      <th>ph</th>\n",
       "      <th>osmo</th>\n",
       "      <th>cond</th>\n",
       "      <th>urea</th>\n",
       "      <th>calc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.734032</td>\n",
       "      <td>0.490334</td>\n",
       "      <td>-0.889728</td>\n",
       "      <td>-0.884388</td>\n",
       "      <td>-1.134869</td>\n",
       "      <td>-0.829136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.015941</td>\n",
       "      <td>1.051532</td>\n",
       "      <td>0.219521</td>\n",
       "      <td>0.288059</td>\n",
       "      <td>2.504767</td>\n",
       "      <td>0.020338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.065904</td>\n",
       "      <td>0.272090</td>\n",
       "      <td>0.526697</td>\n",
       "      <td>0.407969</td>\n",
       "      <td>-0.878040</td>\n",
       "      <td>1.532589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.465925</td>\n",
       "      <td>-1.629751</td>\n",
       "      <td>-0.893994</td>\n",
       "      <td>-1.537228</td>\n",
       "      <td>0.875736</td>\n",
       "      <td>-0.592652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.915909</td>\n",
       "      <td>1.441254</td>\n",
       "      <td>0.949065</td>\n",
       "      <td>-0.484690</td>\n",
       "      <td>0.780342</td>\n",
       "      <td>-0.592652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>1.665882</td>\n",
       "      <td>0.490334</td>\n",
       "      <td>0.859472</td>\n",
       "      <td>-0.005053</td>\n",
       "      <td>0.626245</td>\n",
       "      <td>0.994278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>-1.034021</td>\n",
       "      <td>-0.522942</td>\n",
       "      <td>-1.030517</td>\n",
       "      <td>-0.990974</td>\n",
       "      <td>-0.613873</td>\n",
       "      <td>-0.829136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>1.965871</td>\n",
       "      <td>-0.429409</td>\n",
       "      <td>-1.700332</td>\n",
       "      <td>-1.537228</td>\n",
       "      <td>0.743652</td>\n",
       "      <td>0.116799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>0.465925</td>\n",
       "      <td>-0.756775</td>\n",
       "      <td>-0.463094</td>\n",
       "      <td>-0.005053</td>\n",
       "      <td>0.127262</td>\n",
       "      <td>-0.770015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>-0.134053</td>\n",
       "      <td>-0.351465</td>\n",
       "      <td>-1.004919</td>\n",
       "      <td>1.007515</td>\n",
       "      <td>-0.195608</td>\n",
       "      <td>-0.181917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      gravity        ph      osmo      cond      urea      calc\n",
       "0   -0.734032  0.490334 -0.889728 -0.884388 -1.134869 -0.829136\n",
       "1    0.015941  1.051532  0.219521  0.288059  2.504767  0.020338\n",
       "2    1.065904  0.272090  0.526697  0.407969 -0.878040  1.532589\n",
       "3    0.465925 -1.629751 -0.893994 -1.537228  0.875736 -0.592652\n",
       "4    0.915909  1.441254  0.949065 -0.484690  0.780342 -0.592652\n",
       "..        ...       ...       ...       ...       ...       ...\n",
       "685  1.665882  0.490334  0.859472 -0.005053  0.626245  0.994278\n",
       "686 -1.034021 -0.522942 -1.030517 -0.990974 -0.613873 -0.829136\n",
       "687  1.965871 -0.429409 -1.700332 -1.537228  0.743652  0.116799\n",
       "688  0.465925 -0.756775 -0.463094 -0.005053  0.127262 -0.770015\n",
       "689 -0.134053 -0.351465 -1.004919  1.007515 -0.195608 -0.181917\n",
       "\n",
       "[690 rows x 6 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.5797\n",
      "Epoch 2/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.4160\n",
      "Epoch 3/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.3436\n",
      "Epoch 4/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3137\n",
      "Epoch 5/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.2941\n",
      "Epoch 6/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.2628\n",
      "Epoch 7/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.2423\n",
      "Epoch 8/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.2240\n",
      "Epoch 9/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.2105\n",
      "Epoch 10/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1997\n",
      "Epoch 11/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1835\n",
      "Epoch 12/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1664\n",
      "Epoch 13/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1576\n",
      "Epoch 14/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1468\n",
      "Epoch 15/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1338\n",
      "Epoch 16/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1202\n",
      "Epoch 17/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1155\n",
      "Epoch 18/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1093\n",
      "Epoch 19/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1007\n",
      "Epoch 20/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0927\n",
      "Epoch 21/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0910\n",
      "Epoch 22/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0835\n",
      "Epoch 23/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0806\n",
      "Epoch 24/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0767\n",
      "Epoch 25/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0737\n",
      "Epoch 26/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0736\n",
      "Epoch 27/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0639\n",
      "Epoch 28/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0656\n",
      "Epoch 29/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0601\n",
      "Epoch 30/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0518\n",
      "Epoch 31/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0484\n",
      "Epoch 32/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0443\n",
      "Epoch 33/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0423\n",
      "Epoch 34/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0396\n",
      "Epoch 35/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0390\n",
      "Epoch 36/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0401\n",
      "Epoch 37/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0383\n",
      "Epoch 38/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0345\n",
      "Epoch 39/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0323\n",
      "Epoch 40/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0311\n",
      "Epoch 41/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0285\n",
      "Epoch 42/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0273\n",
      "Epoch 43/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0264\n",
      "Epoch 44/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0248\n",
      "Epoch 45/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0248\n",
      "Epoch 46/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0241\n",
      "Epoch 47/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0229\n",
      "Epoch 48/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0215\n",
      "Epoch 49/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0215\n",
      "Epoch 50/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0203\n",
      "Epoch 51/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0191\n",
      "Epoch 52/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0228\n",
      "Epoch 53/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0227\n",
      "Epoch 54/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0230\n",
      "Epoch 55/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0229\n",
      "Epoch 56/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0227\n",
      "Epoch 57/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0216\n",
      "Epoch 58/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0213\n",
      "Epoch 59/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0195\n",
      "Epoch 60/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0193\n",
      "Epoch 61/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0202\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(noisy_X, X_df, epochs=2001,\n",
    "                    callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "features_df = extract_features(model, X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_df = features_df.iloc[len(X_train_df):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_df = features_df.iloc[:len(X_train_df),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.fit(train_features_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8913043478260869"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lr_model.predict(train_features_df)\n",
    "accuracy_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_model.predict(test_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = test_df.copy()\n",
    "sub_df['target'] = y_pred\n",
    "sub_df = sub_df[['id', 'target']]\n",
    "sub_df.to_csv(os.path.join(proj_dir, 'data', 'submissions', 'dae_sn30_model_submission_2.csv'), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score = 0.78666.  A slight improvement but still under the plain logistic model.  I'll try tuning the swap_noise next"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Swap_noise tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mlflow.create_experiment('swap-probability-tuning')\n",
    "except:\n",
    "    print('experiment already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///c:/Users/eria/OneDrive%20-%20Novozymes%20A%20S/Documents/projects/kaggle/tabular_playground_series/kaggle-playground-series-s3e12/notebooks/mlruns/143698868790863150', creation_time=1681606247421, experiment_id='143698868790863150', last_update_time=1681606247421, lifecycle_stage='active', name='swap-probability-tuning', tags={}>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment('swap-probability-tuning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2842\n",
      "Epoch 2/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1178\n",
      "Epoch 3/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0667\n",
      "Epoch 4/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0449\n",
      "Epoch 5/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0334\n",
      "Epoch 6/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0261\n",
      "Epoch 7/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0214\n",
      "Epoch 8/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0174\n",
      "Epoch 9/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0148\n",
      "Epoch 10/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0124\n",
      "Epoch 11/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0112\n",
      "Epoch 12/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0103\n",
      "Epoch 13/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0094\n",
      "Epoch 14/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0091\n",
      "Epoch 15/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0089\n",
      "Epoch 16/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0081\n",
      "Epoch 17/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0082\n",
      "Epoch 18/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0080\n",
      "Epoch 19/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0081\n",
      "Epoch 20/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0083\n",
      "Epoch 21/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0078\n",
      "Epoch 22/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0074\n",
      "Epoch 23/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0081\n",
      "Epoch 24/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0076\n",
      "Epoch 25/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0069\n",
      "Epoch 26/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0067\n",
      "Epoch 27/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0064\n",
      "Epoch 28/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0058\n",
      "Epoch 29/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0057\n",
      "Epoch 30/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0058\n",
      "Epoch 31/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0056\n",
      "Epoch 32/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0063\n",
      "Epoch 33/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0057\n",
      "Epoch 34/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0052\n",
      "Epoch 35/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0047\n",
      "Epoch 36/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0054\n",
      "Epoch 37/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0055\n",
      "Epoch 38/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0050\n",
      "Epoch 39/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0046\n",
      "Epoch 40/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0037\n",
      "Epoch 41/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0040\n",
      "Epoch 42/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0042\n",
      "Epoch 43/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0043\n",
      "Epoch 44/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0045\n",
      "Epoch 45/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0041\n",
      "Epoch 46/2001\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0048\n",
      "Epoch 47/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0050\n",
      "Epoch 48/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0052\n",
      "Epoch 49/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0052\n",
      "Epoch 50/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0060\n",
      "22/22 [==============================] - 0s 2ms/step\n",
      "Swap probability =  0.01 Train Accuracy Score =  0.9371980676328503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1075\n",
      "Epoch 2/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0590\n",
      "Epoch 3/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0408\n",
      "Epoch 4/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0296\n",
      "Epoch 5/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0240\n",
      "Epoch 6/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0203\n",
      "Epoch 7/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0175\n",
      "Epoch 8/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0155\n",
      "Epoch 9/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0134\n",
      "Epoch 10/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0122\n",
      "Epoch 11/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0114\n",
      "Epoch 12/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0107\n",
      "Epoch 13/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0099\n",
      "Epoch 14/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0093\n",
      "Epoch 15/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0089\n",
      "Epoch 16/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0090\n",
      "Epoch 17/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0093\n",
      "Epoch 18/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0088\n",
      "Epoch 19/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0075\n",
      "Epoch 20/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0079\n",
      "Epoch 21/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0073\n",
      "Epoch 22/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0074\n",
      "Epoch 23/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0072\n",
      "Epoch 24/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0069\n",
      "Epoch 25/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0072\n",
      "Epoch 26/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0073\n",
      "Epoch 27/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0074\n",
      "Epoch 28/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0064\n",
      "Epoch 29/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0063\n",
      "Epoch 30/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0067\n",
      "Epoch 31/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0060\n",
      "Epoch 32/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0060\n",
      "Epoch 33/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0053\n",
      "Epoch 34/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0053\n",
      "Epoch 35/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0051\n",
      "Epoch 36/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0047\n",
      "Epoch 37/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0046\n",
      "Epoch 38/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0047\n",
      "Epoch 39/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0052\n",
      "Epoch 40/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0053\n",
      "Epoch 41/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0051\n",
      "Epoch 42/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0050\n",
      "Epoch 43/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0050\n",
      "Epoch 44/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0048\n",
      "Epoch 45/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0046\n",
      "Epoch 46/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0044\n",
      "Epoch 47/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0041\n",
      "Epoch 48/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0040\n",
      "Epoch 49/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0039\n",
      "Epoch 50/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0038\n",
      "Epoch 51/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0036\n",
      "Epoch 52/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0039\n",
      "Epoch 53/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0046\n",
      "Epoch 54/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0051\n",
      "Epoch 55/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0049\n",
      "Epoch 56/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0051\n",
      "Epoch 57/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0051\n",
      "Epoch 58/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0048\n",
      "Epoch 59/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0054\n",
      "Epoch 60/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0061\n",
      "Epoch 61/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0068\n",
      "22/22 [==============================] - 0s 2ms/step\n",
      "Swap probability =  0.05 Train Accuracy Score =  0.9323671497584541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1514\n",
      "Epoch 2/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0862\n",
      "Epoch 3/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0611\n",
      "Epoch 4/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0476\n",
      "Epoch 5/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0383\n",
      "Epoch 6/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0331\n",
      "Epoch 7/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0266\n",
      "Epoch 8/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0236\n",
      "Epoch 9/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0205\n",
      "Epoch 10/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0178\n",
      "Epoch 11/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0166\n",
      "Epoch 12/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0155\n",
      "Epoch 13/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0138\n",
      "Epoch 14/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0132\n",
      "Epoch 15/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0118\n",
      "Epoch 16/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0113\n",
      "Epoch 17/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0114\n",
      "Epoch 18/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0105\n",
      "Epoch 19/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0097\n",
      "Epoch 20/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0096\n",
      "Epoch 21/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0091\n",
      "Epoch 22/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0081\n",
      "Epoch 23/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0077\n",
      "Epoch 24/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0081\n",
      "Epoch 25/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0079\n",
      "Epoch 26/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0065\n",
      "Epoch 27/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0065\n",
      "Epoch 28/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0064\n",
      "Epoch 29/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0070\n",
      "Epoch 30/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0069\n",
      "Epoch 31/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0072\n",
      "Epoch 32/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0066\n",
      "Epoch 33/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0062\n",
      "Epoch 34/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0061\n",
      "Epoch 35/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0067\n",
      "Epoch 36/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0065\n",
      "Epoch 37/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0056\n",
      "Epoch 38/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0060\n",
      "Epoch 39/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0058\n",
      "Epoch 40/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0055\n",
      "Epoch 41/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0053\n",
      "Epoch 42/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0050\n",
      "Epoch 43/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0051\n",
      "Epoch 44/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0057\n",
      "Epoch 45/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0051\n",
      "Epoch 46/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0048\n",
      "Epoch 47/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0048\n",
      "Epoch 48/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0047\n",
      "Epoch 49/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0044\n",
      "Epoch 50/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0045\n",
      "Epoch 51/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0045\n",
      "Epoch 52/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0045\n",
      "Epoch 53/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0045\n",
      "Epoch 54/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0045\n",
      "Epoch 55/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0041\n",
      "Epoch 56/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0041\n",
      "Epoch 57/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0040\n",
      "Epoch 58/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0039\n",
      "Epoch 59/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0041\n",
      "Epoch 60/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0043\n",
      "Epoch 61/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0045\n",
      "Epoch 62/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0043\n",
      "Epoch 63/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0044\n",
      "Epoch 64/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0041\n",
      "Epoch 65/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0041\n",
      "Epoch 66/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0040\n",
      "Epoch 67/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0039\n",
      "Epoch 68/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0038\n",
      "Epoch 69/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0041\n",
      "Epoch 70/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0042\n",
      "Epoch 71/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0041\n",
      "Epoch 72/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0046\n",
      "Epoch 73/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0042\n",
      "Epoch 74/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0036\n",
      "Epoch 75/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0035\n",
      "Epoch 76/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0036\n",
      "Epoch 77/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0038\n",
      "Epoch 78/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0037\n",
      "Epoch 79/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0037\n",
      "Epoch 80/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0042\n",
      "Epoch 81/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0047\n",
      "Epoch 82/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0044\n",
      "Epoch 83/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0043\n",
      "Epoch 84/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0039\n",
      "Epoch 85/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0039\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Swap probability =  0.1 Train Accuracy Score =  0.9444444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2435\n",
      "Epoch 2/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1520\n",
      "Epoch 3/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1136\n",
      "Epoch 4/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0903\n",
      "Epoch 5/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0745\n",
      "Epoch 6/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0619\n",
      "Epoch 7/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0515\n",
      "Epoch 8/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0439\n",
      "Epoch 9/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0374\n",
      "Epoch 10/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0316\n",
      "Epoch 11/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0275\n",
      "Epoch 12/2001\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0243\n",
      "Epoch 13/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0220\n",
      "Epoch 14/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0199\n",
      "Epoch 15/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0184\n",
      "Epoch 16/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0168\n",
      "Epoch 17/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0153\n",
      "Epoch 18/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0149\n",
      "Epoch 19/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0135\n",
      "Epoch 20/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0130\n",
      "Epoch 21/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0121\n",
      "Epoch 22/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0125\n",
      "Epoch 23/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0113\n",
      "Epoch 24/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0110\n",
      "Epoch 25/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0109\n",
      "Epoch 26/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0110\n",
      "Epoch 27/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0126\n",
      "Epoch 28/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0114\n",
      "Epoch 29/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0115\n",
      "Epoch 30/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0116\n",
      "Epoch 31/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0107\n",
      "Epoch 32/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0104\n",
      "Epoch 33/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0101\n",
      "Epoch 34/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0094\n",
      "Epoch 35/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0094\n",
      "Epoch 36/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0091\n",
      "Epoch 37/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0088\n",
      "Epoch 38/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0091\n",
      "Epoch 39/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0078\n",
      "Epoch 40/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0067\n",
      "Epoch 41/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0059\n",
      "Epoch 42/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0060\n",
      "Epoch 43/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0054\n",
      "Epoch 44/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0057\n",
      "Epoch 45/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0061\n",
      "Epoch 46/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0060\n",
      "Epoch 47/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0051\n",
      "Epoch 48/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0044\n",
      "Epoch 49/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0043\n",
      "Epoch 50/2001\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0044\n",
      "Epoch 51/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0045\n",
      "Epoch 52/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0046\n",
      "Epoch 53/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0046\n",
      "Epoch 54/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0049\n",
      "Epoch 55/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0046\n",
      "Epoch 56/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0046\n",
      "Epoch 57/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0054\n",
      "Epoch 58/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0066\n",
      "Epoch 59/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0062\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Swap probability =  0.15 Train Accuracy Score =  0.9347826086956522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3322\n",
      "Epoch 2/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2152\n",
      "Epoch 3/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1653\n",
      "Epoch 4/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.1336\n",
      "Epoch 5/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.1098\n",
      "Epoch 6/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0921\n",
      "Epoch 7/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0767\n",
      "Epoch 8/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0654\n",
      "Epoch 9/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0571\n",
      "Epoch 10/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0489\n",
      "Epoch 11/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0431\n",
      "Epoch 12/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0394\n",
      "Epoch 13/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0340\n",
      "Epoch 14/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0299\n",
      "Epoch 15/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0277\n",
      "Epoch 16/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0262\n",
      "Epoch 17/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0240\n",
      "Epoch 18/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0224\n",
      "Epoch 19/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0216\n",
      "Epoch 20/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0203\n",
      "Epoch 21/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0186\n",
      "Epoch 22/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0171\n",
      "Epoch 23/2001\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0168\n",
      "Epoch 24/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0162\n",
      "Epoch 25/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0157\n",
      "Epoch 26/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0146\n",
      "Epoch 27/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0145\n",
      "Epoch 28/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0142\n",
      "Epoch 29/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0130\n",
      "Epoch 30/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0121\n",
      "Epoch 31/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0119\n",
      "Epoch 32/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0119\n",
      "Epoch 33/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0122\n",
      "Epoch 34/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0114\n",
      "Epoch 35/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0102\n",
      "Epoch 36/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0096\n",
      "Epoch 37/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0092\n",
      "Epoch 38/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0090\n",
      "Epoch 39/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0090\n",
      "Epoch 40/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0095\n",
      "Epoch 41/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0093\n",
      "Epoch 42/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0109\n",
      "Epoch 43/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0108\n",
      "Epoch 44/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0105\n",
      "Epoch 45/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0106\n",
      "Epoch 46/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0113\n",
      "Epoch 47/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0105\n",
      "Epoch 48/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0095\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Swap probability =  0.2 Train Accuracy Score =  0.9396135265700483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.4285\n",
      "Epoch 2/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2657\n",
      "Epoch 3/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2057\n",
      "Epoch 4/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1654\n",
      "Epoch 5/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1361\n",
      "Epoch 6/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.1126\n",
      "Epoch 7/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0950\n",
      "Epoch 8/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0815\n",
      "Epoch 9/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0695\n",
      "Epoch 10/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0597\n",
      "Epoch 11/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0521\n",
      "Epoch 12/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0458\n",
      "Epoch 13/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0400\n",
      "Epoch 14/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0357\n",
      "Epoch 15/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0314\n",
      "Epoch 16/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0278\n",
      "Epoch 17/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0251\n",
      "Epoch 18/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0229\n",
      "Epoch 19/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0207\n",
      "Epoch 20/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0197\n",
      "Epoch 21/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0193\n",
      "Epoch 22/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0193\n",
      "Epoch 23/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0189\n",
      "Epoch 24/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0190\n",
      "Epoch 25/2001\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0174\n",
      "Epoch 26/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0174\n",
      "Epoch 27/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0161\n",
      "Epoch 28/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0143\n",
      "Epoch 29/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0136\n",
      "Epoch 30/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0135\n",
      "Epoch 31/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0127\n",
      "Epoch 32/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0118\n",
      "Epoch 33/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0120\n",
      "Epoch 34/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0127\n",
      "Epoch 35/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0121\n",
      "Epoch 36/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0119\n",
      "Epoch 37/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0115\n",
      "Epoch 38/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0116\n",
      "Epoch 39/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0117\n",
      "Epoch 40/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0108\n",
      "Epoch 41/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0097\n",
      "Epoch 42/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0098\n",
      "Epoch 43/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0100\n",
      "Epoch 44/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0101\n",
      "Epoch 45/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0098\n",
      "Epoch 46/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0099\n",
      "Epoch 47/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0104\n",
      "Epoch 48/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0108\n",
      "Epoch 49/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0100\n",
      "Epoch 50/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0106\n",
      "Epoch 51/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0104\n",
      "22/22 [==============================] - 0s 2ms/step\n",
      "Swap probability =  0.25 Train Accuracy Score =  0.9468599033816425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.5180\n",
      "Epoch 2/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3315\n",
      "Epoch 3/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2562\n",
      "Epoch 4/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2099\n",
      "Epoch 5/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1759\n",
      "Epoch 6/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.1479\n",
      "Epoch 7/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1260\n",
      "Epoch 8/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1087\n",
      "Epoch 9/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0927\n",
      "Epoch 10/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0807\n",
      "Epoch 11/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0693\n",
      "Epoch 12/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0601\n",
      "Epoch 13/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0527\n",
      "Epoch 14/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0463\n",
      "Epoch 15/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0416\n",
      "Epoch 16/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0371\n",
      "Epoch 17/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0335\n",
      "Epoch 18/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0303\n",
      "Epoch 19/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0278\n",
      "Epoch 20/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0254\n",
      "Epoch 21/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0240\n",
      "Epoch 22/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0219\n",
      "Epoch 23/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0202\n",
      "Epoch 24/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0193\n",
      "Epoch 25/2001\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0174\n",
      "Epoch 26/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0164\n",
      "Epoch 27/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0160\n",
      "Epoch 28/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0158\n",
      "Epoch 29/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0166\n",
      "Epoch 30/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0154\n",
      "Epoch 31/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0150\n",
      "Epoch 32/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0143\n",
      "Epoch 33/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0135\n",
      "Epoch 34/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0134\n",
      "Epoch 35/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0126\n",
      "Epoch 36/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0118\n",
      "Epoch 37/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0117\n",
      "Epoch 38/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0112\n",
      "Epoch 39/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0108\n",
      "Epoch 40/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0113\n",
      "Epoch 41/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0110\n",
      "Epoch 42/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0104\n",
      "Epoch 43/2001\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0105\n",
      "Epoch 44/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0098\n",
      "Epoch 45/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0106\n",
      "Epoch 46/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0115\n",
      "Epoch 47/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0104\n",
      "Epoch 48/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0102\n",
      "Epoch 49/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0104\n",
      "Epoch 50/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0100\n",
      "Epoch 51/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0096\n",
      "Epoch 52/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0101\n",
      "Epoch 53/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0100\n",
      "Epoch 54/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0092\n",
      "Epoch 55/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0086\n",
      "Epoch 56/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0092\n",
      "Epoch 57/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0088\n",
      "Epoch 58/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0096\n",
      "Epoch 59/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0095\n",
      "Epoch 60/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0087\n",
      "Epoch 61/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0084\n",
      "Epoch 62/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0070\n",
      "Epoch 63/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0072\n",
      "Epoch 64/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0076\n",
      "Epoch 65/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0082\n",
      "Epoch 66/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0085\n",
      "Epoch 67/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0075\n",
      "Epoch 68/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0066\n",
      "Epoch 69/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0073\n",
      "Epoch 70/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0078\n",
      "Epoch 71/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0073\n",
      "Epoch 72/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0075\n",
      "Epoch 73/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0072\n",
      "Epoch 74/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0072\n",
      "Epoch 75/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0081\n",
      "Epoch 76/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0073\n",
      "Epoch 77/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0068\n",
      "Epoch 78/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0063\n",
      "Epoch 79/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0055\n",
      "Epoch 80/2001\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0051\n",
      "Epoch 81/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0047\n",
      "Epoch 82/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0049\n",
      "Epoch 83/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0049\n",
      "Epoch 84/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0046\n",
      "Epoch 85/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0053\n",
      "Epoch 86/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0057\n",
      "Epoch 87/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0061\n",
      "Epoch 88/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0063\n",
      "Epoch 89/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0060\n",
      "Epoch 90/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0065\n",
      "Epoch 91/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0073\n",
      "Epoch 92/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0087\n",
      "Epoch 93/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0100\n",
      "Epoch 94/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0104\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Swap probability =  0.3 Train Accuracy Score =  0.9516908212560387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.6126\n",
      "Epoch 2/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3963\n",
      "Epoch 3/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3106\n",
      "Epoch 4/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2551\n",
      "Epoch 5/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2128\n",
      "Epoch 6/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1818\n",
      "Epoch 7/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1578\n",
      "Epoch 8/2001\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.1361\n",
      "Epoch 9/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.1186\n",
      "Epoch 10/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1035\n",
      "Epoch 11/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0907\n",
      "Epoch 12/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0794\n",
      "Epoch 13/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0696\n",
      "Epoch 14/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0622\n",
      "Epoch 15/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0549\n",
      "Epoch 16/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0491\n",
      "Epoch 17/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0442\n",
      "Epoch 18/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0393\n",
      "Epoch 19/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0360\n",
      "Epoch 20/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0334\n",
      "Epoch 21/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0315\n",
      "Epoch 22/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0296\n",
      "Epoch 23/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0281\n",
      "Epoch 24/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0261\n",
      "Epoch 25/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0251\n",
      "Epoch 26/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0229\n",
      "Epoch 27/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0213\n",
      "Epoch 28/2001\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0196\n",
      "Epoch 29/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0174\n",
      "Epoch 30/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0170\n",
      "Epoch 31/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0181\n",
      "Epoch 32/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0184\n",
      "Epoch 33/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0176\n",
      "Epoch 34/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0165\n",
      "Epoch 35/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0150\n",
      "Epoch 36/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0144\n",
      "Epoch 37/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0134\n",
      "Epoch 38/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0124\n",
      "Epoch 39/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0115\n",
      "Epoch 40/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0112\n",
      "Epoch 41/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0146\n",
      "Epoch 42/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0144\n",
      "Epoch 43/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0141\n",
      "Epoch 44/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0143\n",
      "Epoch 45/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0146\n",
      "Epoch 46/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0152\n",
      "Epoch 47/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0137\n",
      "Epoch 48/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0137\n",
      "Epoch 49/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0141\n",
      "Epoch 50/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0133\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Swap probability =  0.35 Train Accuracy Score =  0.9444444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2001\n",
      "22/22 [==============================] - 1s 4ms/step - loss: 0.7311\n",
      "Epoch 2/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.4495\n",
      "Epoch 3/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3534\n",
      "Epoch 4/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2928\n",
      "Epoch 5/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2456\n",
      "Epoch 6/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2108\n",
      "Epoch 7/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.1823\n",
      "Epoch 8/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1589\n",
      "Epoch 9/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1413\n",
      "Epoch 10/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1244\n",
      "Epoch 11/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1109\n",
      "Epoch 12/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0984\n",
      "Epoch 13/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0903\n",
      "Epoch 14/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0795\n",
      "Epoch 15/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0700\n",
      "Epoch 16/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0642\n",
      "Epoch 17/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0566\n",
      "Epoch 18/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0509\n",
      "Epoch 19/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0471\n",
      "Epoch 20/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0421\n",
      "Epoch 21/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0378\n",
      "Epoch 22/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0356\n",
      "Epoch 23/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0335\n",
      "Epoch 24/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0326\n",
      "Epoch 25/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0303\n",
      "Epoch 26/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0281\n",
      "Epoch 27/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0258\n",
      "Epoch 28/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0251\n",
      "Epoch 29/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0232\n",
      "Epoch 30/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0242\n",
      "Epoch 31/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0237\n",
      "Epoch 32/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0228\n",
      "Epoch 33/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0222\n",
      "Epoch 34/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0222\n",
      "Epoch 35/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0229\n",
      "Epoch 36/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0215\n",
      "Epoch 37/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0205\n",
      "Epoch 38/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0211\n",
      "Epoch 39/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0199\n",
      "Epoch 40/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0194\n",
      "Epoch 41/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0210\n",
      "Epoch 42/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0191\n",
      "Epoch 43/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0182\n",
      "Epoch 44/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0196\n",
      "Epoch 45/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0182\n",
      "Epoch 46/2001\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0160\n",
      "Epoch 47/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0136\n",
      "Epoch 48/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0140\n",
      "Epoch 49/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0134\n",
      "Epoch 50/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0130\n",
      "Epoch 51/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0130\n",
      "Epoch 52/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0122\n",
      "Epoch 53/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0123\n",
      "Epoch 54/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0118\n",
      "Epoch 55/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0116\n",
      "Epoch 56/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0109\n",
      "Epoch 57/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0108\n",
      "Epoch 58/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0104\n",
      "Epoch 59/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0109\n",
      "Epoch 60/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0116\n",
      "Epoch 61/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0114\n",
      "Epoch 62/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0101\n",
      "Epoch 63/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0107\n",
      "Epoch 64/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0117\n",
      "Epoch 65/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0130\n",
      "Epoch 66/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0139\n",
      "Epoch 67/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0132\n",
      "Epoch 68/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0120\n",
      "Epoch 69/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0121\n",
      "Epoch 70/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0126\n",
      "Epoch 71/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0129\n",
      "Epoch 72/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0121\n",
      "22/22 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swap probability =  0.4 Train Accuracy Score =  0.9444444444444444\n",
      "Epoch 1/2001\n",
      "22/22 [==============================] - 1s 5ms/step - loss: 0.9335\n",
      "Epoch 2/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.5966\n",
      "Epoch 3/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.4713\n",
      "Epoch 4/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3908\n",
      "Epoch 5/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.3309\n",
      "Epoch 6/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.2843\n",
      "Epoch 7/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2458\n",
      "Epoch 8/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2137\n",
      "Epoch 9/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1861\n",
      "Epoch 10/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1636\n",
      "Epoch 11/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1461\n",
      "Epoch 12/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1285\n",
      "Epoch 13/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1128\n",
      "Epoch 14/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1012\n",
      "Epoch 15/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0896\n",
      "Epoch 16/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0799\n",
      "Epoch 17/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0722\n",
      "Epoch 18/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0655\n",
      "Epoch 19/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0600\n",
      "Epoch 20/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0541\n",
      "Epoch 21/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0495\n",
      "Epoch 22/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0435\n",
      "Epoch 23/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0398\n",
      "Epoch 24/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0358\n",
      "Epoch 25/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0339\n",
      "Epoch 26/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0317\n",
      "Epoch 27/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0294\n",
      "Epoch 28/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0290\n",
      "Epoch 29/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0271\n",
      "Epoch 30/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0264\n",
      "Epoch 31/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0256\n",
      "Epoch 32/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0245\n",
      "Epoch 33/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0247\n",
      "Epoch 34/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0249\n",
      "Epoch 35/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0265\n",
      "Epoch 36/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0269\n",
      "Epoch 37/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0252\n",
      "Epoch 38/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0215\n",
      "Epoch 39/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0195\n",
      "Epoch 40/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0191\n",
      "Epoch 41/2001\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0180\n",
      "Epoch 42/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0156\n",
      "Epoch 43/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0154\n",
      "Epoch 44/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0155\n",
      "Epoch 45/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0156\n",
      "Epoch 46/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0174\n",
      "Epoch 47/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0174\n",
      "Epoch 48/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0156\n",
      "Epoch 49/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0131\n",
      "Epoch 50/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0130\n",
      "Epoch 51/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0132\n",
      "Epoch 52/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0130\n",
      "Epoch 53/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0132\n",
      "Epoch 54/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0185\n",
      "Epoch 55/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0210\n",
      "Epoch 56/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0206\n",
      "Epoch 57/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0206\n",
      "Epoch 58/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0212\n",
      "Epoch 59/2001\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0196\n",
      "Epoch 60/2001\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0168\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "Swap probability =  0.5 Train Accuracy Score =  0.9347826086956522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "noise_p_to_test = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5]\n",
    "\n",
    "for p in noise_p_to_test:\n",
    "    with mlflow.start_run():\n",
    "        #reset the model\n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
    "        noisy_X = add_noise(X_df, p=p)\n",
    "        history = model.fit(noisy_X, X_df, epochs=2001,\n",
    "                        callbacks=[early_stopping_cb])\n",
    "        features_df = extract_features(model, X_df)\n",
    "        train_features_df = features_df.iloc[:len(X_train_df),:]\n",
    "        test_features_df = features_df.iloc[len(X_train_df):,:]\n",
    "        lr_model.fit(train_features_df, y_train)\n",
    "        y_train_pred = lr_model.predict(train_features_df)\n",
    "        train_accuracy_score = accuracy_score(y_train, y_train_pred)\n",
    "        print('Swap probability = ', p, 'Train Accuracy Score = ', train_accuracy_score)\n",
    "        mlflow.log_metric('train_accuracy_score', train_accuracy_score)\n",
    "        mlflow.log_param('swap_probability', p)\n",
    "        mlflow.sklearn.log_model(lr_model, 'lr_model_p='+str(p))\n",
    "\n",
    "        y_test_pred = lr_model.predict(test_features_df)\n",
    "        sub_df = test_df.copy()\n",
    "        sub_df['target'] = y_test_pred\n",
    "        sub_df = sub_df[['id', 'target']]\n",
    "        sub_df.to_csv(os.path.join(proj_dir, 'data', 'submissions', 'dae_sn'+str(p)+'_model_submission.csv'), index=False)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
