{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nb_utils\n",
    "proj_dir = nb_utils.proj_path_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(proj_dir, 'data', 'raw', 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(proj_dir, 'data', 'raw', 'test.csv'))\n",
    "feature_cols = [col for col in test_df.columns if col != 'id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(train_df[feature_cols])\n",
    "\n",
    "y_train = np.array(train_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.73403157,  0.36562268, -0.8897276 , -0.88438794, -1.13486936,\n",
       "        -0.82913603],\n",
       "       [ 1.06590369, -0.86589692,  0.21952069,  0.28805926,  0.84638411,\n",
       "         0.02033832],\n",
       "       [-1.33400999,  0.27208954, -1.19690404,  0.40796863, -0.8780402 ,\n",
       "         1.53258937],\n",
       "       ...,\n",
       "       [ 0.01594146,  0.50592238, -0.83853152,  0.10153357, -0.06352489,\n",
       "         1.109408  ],\n",
       "       [-1.48400459,  1.81538625, -1.39315566, -1.17749974, -1.49443017,\n",
       "        -0.9598244 ],\n",
       "       [-1.03402078,  0.27208954, -1.22676842, -1.53722786, -0.8780402 ,\n",
       "        -1.1963081 ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add noise to X data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.dae import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train, columns=feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_X = add_noise(X_train_df, p=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gravity</th>\n",
       "      <th>ph</th>\n",
       "      <th>osmo</th>\n",
       "      <th>cond</th>\n",
       "      <th>urea</th>\n",
       "      <th>calc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.734032</td>\n",
       "      <td>0.365623</td>\n",
       "      <td>-0.889728</td>\n",
       "      <td>-0.884388</td>\n",
       "      <td>-0.878040</td>\n",
       "      <td>-0.829136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.065904</td>\n",
       "      <td>-0.865897</td>\n",
       "      <td>0.219521</td>\n",
       "      <td>0.288059</td>\n",
       "      <td>0.846384</td>\n",
       "      <td>1.096961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.315931</td>\n",
       "      <td>0.272090</td>\n",
       "      <td>-1.196904</td>\n",
       "      <td>0.407969</td>\n",
       "      <td>-0.878040</td>\n",
       "      <td>1.532589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.484005</td>\n",
       "      <td>-1.629751</td>\n",
       "      <td>-0.893994</td>\n",
       "      <td>-0.084992</td>\n",
       "      <td>0.875736</td>\n",
       "      <td>-0.334387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.465925</td>\n",
       "      <td>-0.663242</td>\n",
       "      <td>-1.226768</td>\n",
       "      <td>-0.484690</td>\n",
       "      <td>0.780342</td>\n",
       "      <td>-0.592652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>-1.034021</td>\n",
       "      <td>-1.162085</td>\n",
       "      <td>-0.531355</td>\n",
       "      <td>-0.005053</td>\n",
       "      <td>-1.494430</td>\n",
       "      <td>-0.804243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>0.915909</td>\n",
       "      <td>-0.663242</td>\n",
       "      <td>-1.410221</td>\n",
       "      <td>-1.350702</td>\n",
       "      <td>-0.401072</td>\n",
       "      <td>-0.502415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>0.015941</td>\n",
       "      <td>0.505922</td>\n",
       "      <td>0.070199</td>\n",
       "      <td>0.101534</td>\n",
       "      <td>-0.063525</td>\n",
       "      <td>-0.885145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>-1.484005</td>\n",
       "      <td>1.815386</td>\n",
       "      <td>-1.393156</td>\n",
       "      <td>-1.177500</td>\n",
       "      <td>-0.966096</td>\n",
       "      <td>2.665222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>-1.034021</td>\n",
       "      <td>0.272090</td>\n",
       "      <td>-1.226768</td>\n",
       "      <td>-0.005053</td>\n",
       "      <td>-0.878040</td>\n",
       "      <td>-1.196308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>414 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      gravity        ph      osmo      cond      urea      calc\n",
       "0   -0.734032  0.365623 -0.889728 -0.884388 -0.878040 -0.829136\n",
       "1    1.065904 -0.865897  0.219521  0.288059  0.846384  1.096961\n",
       "2    0.315931  0.272090 -1.196904  0.407969 -0.878040  1.532589\n",
       "3   -1.484005 -1.629751 -0.893994 -0.084992  0.875736 -0.334387\n",
       "4    0.465925 -0.663242 -1.226768 -0.484690  0.780342 -0.592652\n",
       "..        ...       ...       ...       ...       ...       ...\n",
       "409 -1.034021 -1.162085 -0.531355 -0.005053 -1.494430 -0.804243\n",
       "410  0.915909 -0.663242 -1.410221 -1.350702 -0.401072 -0.502415\n",
       "411  0.015941  0.505922  0.070199  0.101534 -0.063525 -0.885145\n",
       "412 -1.484005  1.815386 -1.393156 -1.177500 -0.966096  2.665222\n",
       "413 -1.034021  0.272090 -1.226768 -0.005053 -0.878040 -1.196308\n",
       "\n",
       "[414 rows x 6 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gravity</th>\n",
       "      <th>ph</th>\n",
       "      <th>osmo</th>\n",
       "      <th>cond</th>\n",
       "      <th>urea</th>\n",
       "      <th>calc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.013</td>\n",
       "      <td>6.19</td>\n",
       "      <td>443</td>\n",
       "      <td>14.8</td>\n",
       "      <td>124</td>\n",
       "      <td>1.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.025</td>\n",
       "      <td>5.40</td>\n",
       "      <td>703</td>\n",
       "      <td>23.6</td>\n",
       "      <td>394</td>\n",
       "      <td>4.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.009</td>\n",
       "      <td>6.13</td>\n",
       "      <td>371</td>\n",
       "      <td>24.5</td>\n",
       "      <td>159</td>\n",
       "      <td>9.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.021</td>\n",
       "      <td>4.91</td>\n",
       "      <td>442</td>\n",
       "      <td>20.8</td>\n",
       "      <td>398</td>\n",
       "      <td>6.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.021</td>\n",
       "      <td>5.53</td>\n",
       "      <td>874</td>\n",
       "      <td>17.8</td>\n",
       "      <td>385</td>\n",
       "      <td>2.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>1.011</td>\n",
       "      <td>5.21</td>\n",
       "      <td>527</td>\n",
       "      <td>21.4</td>\n",
       "      <td>75</td>\n",
       "      <td>1.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>1.024</td>\n",
       "      <td>5.53</td>\n",
       "      <td>577</td>\n",
       "      <td>19.7</td>\n",
       "      <td>224</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1.018</td>\n",
       "      <td>6.28</td>\n",
       "      <td>455</td>\n",
       "      <td>22.2</td>\n",
       "      <td>270</td>\n",
       "      <td>7.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1.008</td>\n",
       "      <td>7.12</td>\n",
       "      <td>325</td>\n",
       "      <td>12.6</td>\n",
       "      <td>75</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1.011</td>\n",
       "      <td>6.13</td>\n",
       "      <td>364</td>\n",
       "      <td>9.9</td>\n",
       "      <td>159</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>414 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gravity    ph  osmo  cond  urea  calc\n",
       "0      1.013  6.19   443  14.8   124  1.45\n",
       "1      1.025  5.40   703  23.6   394  4.18\n",
       "2      1.009  6.13   371  24.5   159  9.04\n",
       "3      1.021  4.91   442  20.8   398  6.63\n",
       "4      1.021  5.53   874  17.8   385  2.21\n",
       "..       ...   ...   ...   ...   ...   ...\n",
       "409    1.011  5.21   527  21.4    75  1.53\n",
       "410    1.024  5.53   577  19.7   224  0.77\n",
       "411    1.018  6.28   455  22.2   270  7.68\n",
       "412    1.008  7.12   325  12.6    75  1.03\n",
       "413    1.011  6.13   364   9.9   159  0.27\n",
       "\n",
       "[414 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=noisy_X.shape[1:]),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dense(6)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.6424\n",
      "Epoch 2/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4575\n",
      "Epoch 3/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4161\n",
      "Epoch 4/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.4009\n",
      "Epoch 5/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3877\n",
      "Epoch 6/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3749\n",
      "Epoch 7/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3613\n",
      "Epoch 8/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3583\n",
      "Epoch 9/2001\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.3487\n",
      "Epoch 10/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.3231\n",
      "Epoch 11/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.3073\n",
      "Epoch 12/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.2916\n",
      "Epoch 13/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2803\n",
      "Epoch 14/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2682\n",
      "Epoch 15/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2584\n",
      "Epoch 16/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2601\n",
      "Epoch 17/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2481\n",
      "Epoch 18/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2480\n",
      "Epoch 19/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2352\n",
      "Epoch 20/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2278\n",
      "Epoch 21/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2173\n",
      "Epoch 22/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.2000\n",
      "Epoch 23/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1946\n",
      "Epoch 24/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1909\n",
      "Epoch 25/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1769\n",
      "Epoch 26/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1706\n",
      "Epoch 27/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1630\n",
      "Epoch 28/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1568\n",
      "Epoch 29/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1492\n",
      "Epoch 30/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1410\n",
      "Epoch 31/2001\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.1399\n",
      "Epoch 32/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1336\n",
      "Epoch 33/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.1291\n",
      "Epoch 34/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1177\n",
      "Epoch 35/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1194\n",
      "Epoch 36/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1094\n",
      "Epoch 37/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1101\n",
      "Epoch 38/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1072\n",
      "Epoch 39/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1081\n",
      "Epoch 40/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.1032\n",
      "Epoch 41/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0960\n",
      "Epoch 42/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0884\n",
      "Epoch 43/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0859\n",
      "Epoch 44/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0848\n",
      "Epoch 45/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0772\n",
      "Epoch 46/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0761\n",
      "Epoch 47/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0670\n",
      "Epoch 48/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0651\n",
      "Epoch 49/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0615\n",
      "Epoch 50/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0615\n",
      "Epoch 51/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0578\n",
      "Epoch 52/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0635\n",
      "Epoch 53/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0582\n",
      "Epoch 54/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0489\n",
      "Epoch 55/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0487\n",
      "Epoch 56/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0481\n",
      "Epoch 57/2001\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0465\n",
      "Epoch 58/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0462\n",
      "Epoch 59/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0425\n",
      "Epoch 60/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0424\n",
      "Epoch 61/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0410\n",
      "Epoch 62/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0448\n",
      "Epoch 63/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0437\n",
      "Epoch 64/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0404\n",
      "Epoch 65/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0380\n",
      "Epoch 66/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0372\n",
      "Epoch 67/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0358\n",
      "Epoch 68/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0298\n",
      "Epoch 69/2001\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0278\n",
      "Epoch 70/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0278\n",
      "Epoch 71/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0285\n",
      "Epoch 72/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0288\n",
      "Epoch 73/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0317\n",
      "Epoch 74/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0309\n",
      "Epoch 75/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0298\n",
      "Epoch 76/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0299\n",
      "Epoch 77/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0263\n",
      "Epoch 78/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0236\n",
      "Epoch 79/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0202\n",
      "Epoch 80/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0186\n",
      "Epoch 81/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0192\n",
      "Epoch 82/2001\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 0.0208\n",
      "Epoch 83/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0207\n",
      "Epoch 84/2001\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0185\n",
      "Epoch 85/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0186\n",
      "Epoch 86/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0221\n",
      "Epoch 87/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0197\n",
      "Epoch 88/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0195\n",
      "Epoch 89/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0175\n",
      "Epoch 90/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0155\n",
      "Epoch 91/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0163\n",
      "Epoch 92/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0147\n",
      "Epoch 93/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0124\n",
      "Epoch 94/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0136\n",
      "Epoch 95/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0140\n",
      "Epoch 96/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0143\n",
      "Epoch 97/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0143\n",
      "Epoch 98/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0140\n",
      "Epoch 99/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0119\n",
      "Epoch 100/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0105\n",
      "Epoch 101/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0103\n",
      "Epoch 102/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0103\n",
      "Epoch 103/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0105\n",
      "Epoch 104/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0093\n",
      "Epoch 105/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0093\n",
      "Epoch 106/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0098\n",
      "Epoch 107/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0103\n",
      "Epoch 108/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0100\n",
      "Epoch 109/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0104\n",
      "Epoch 110/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0107\n",
      "Epoch 111/2001\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 0.0100\n",
      "Epoch 112/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0098\n",
      "Epoch 113/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0116\n",
      "Epoch 114/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0119\n",
      "Epoch 115/2001\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 0.0154\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(noisy_X, X_train_df, epochs=2001,\n",
    "                    callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(proj_dir, 'models', 'dae_model_swap_noise_30.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "features_df = extract_features(model, X_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1490</th>\n",
       "      <th>1491</th>\n",
       "      <th>1492</th>\n",
       "      <th>1493</th>\n",
       "      <th>1494</th>\n",
       "      <th>1495</th>\n",
       "      <th>1496</th>\n",
       "      <th>1497</th>\n",
       "      <th>1498</th>\n",
       "      <th>1499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.105094</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055305</td>\n",
       "      <td>0.239354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.209187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.511415</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.100698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063757</td>\n",
       "      <td>0.149102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213341</td>\n",
       "      <td>0.274871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133355</td>\n",
       "      <td>0.273627</td>\n",
       "      <td>0.027265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.211943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097726</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252990</td>\n",
       "      <td>0.018421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.579527</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.265349</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.140334</td>\n",
       "      <td>0.061624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>0.185923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247766</td>\n",
       "      <td>0.232137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.502687</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307541</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020367</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>0.281536</td>\n",
       "      <td>0.044350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070411</td>\n",
       "      <td>0.184628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>0.271684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006511</td>\n",
       "      <td>0.132825</td>\n",
       "      <td>0.087225</td>\n",
       "      <td>0.180131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038580</td>\n",
       "      <td>0.106261</td>\n",
       "      <td>0.044047</td>\n",
       "      <td>0.114512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026933</td>\n",
       "      <td>0.193787</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097417</td>\n",
       "      <td>0.020793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172312</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052521</td>\n",
       "      <td>0.225045</td>\n",
       "      <td>0.418127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.390289</td>\n",
       "      <td>0.093095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.474712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.890431</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0.084376</td>\n",
       "      <td>0.011557</td>\n",
       "      <td>0.077660</td>\n",
       "      <td>0.206143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222388</td>\n",
       "      <td>0.011962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.484163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.556341</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>414 rows × 1500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6     \\\n",
       "0    0.105094  0.000000  0.055305  0.239354  0.000000  0.000000  0.209187   \n",
       "1    0.100698  0.000000  0.000000  0.000000  0.063757  0.149102  0.000000   \n",
       "2    0.000000  0.211943  0.000000  0.097726  0.000000  0.000000  0.252990   \n",
       "3    0.140334  0.061624  0.000000  0.000000  0.001513  0.185923  0.000000   \n",
       "4    0.008934  0.000000  0.000000  0.000000  0.000000  0.092718  0.000000   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "409  0.281536  0.044350  0.000000  0.164553  0.000000  0.070411  0.184628   \n",
       "410  0.271684  0.000000  0.006511  0.132825  0.087225  0.180131  0.000000   \n",
       "411  0.000000  0.038580  0.106261  0.044047  0.114512  0.000000  0.026933   \n",
       "412  0.000000  0.052521  0.225045  0.418127  0.000000  0.000000  0.390289   \n",
       "413  0.084376  0.011557  0.077660  0.206143  0.000000  0.000000  0.222388   \n",
       "\n",
       "         7     8         9     ...      1490      1491  1492  1493      1494  \\\n",
       "0    0.000000   0.0  0.000000  ...  0.000000  0.000000   0.0   0.0  0.372682   \n",
       "1    0.046329   0.0  0.000000  ...  0.213341  0.274871   0.0   0.0  0.000000   \n",
       "2    0.018421   0.0  0.000000  ...  0.579527  0.000000   0.0   0.0  0.000000   \n",
       "3    0.115523   0.0  0.000000  ...  0.247766  0.232137   0.0   0.0  0.000000   \n",
       "4    0.000000   0.0  0.135379  ...  0.000000  0.307541   0.0   0.0  0.000000   \n",
       "..        ...   ...       ...  ...       ...       ...   ...   ...       ...   \n",
       "409  0.000000   0.0  0.000000  ...  0.000000  0.000000   0.0   0.0  0.000000   \n",
       "410  0.000000   0.0  0.000000  ...  0.000000  0.000000   0.0   0.0  0.000000   \n",
       "411  0.193787   0.0  0.000000  ...  0.000000  0.000000   0.0   0.0  0.000000   \n",
       "412  0.093095   0.0  0.027577  ...  0.000000  0.000000   0.0   0.0  0.474712   \n",
       "413  0.011962   0.0  0.003069  ...  0.000000  0.000000   0.0   0.0  0.484163   \n",
       "\n",
       "         1495      1496      1497      1498  1499  \n",
       "0    0.000000  0.000000  0.000000  0.511415   0.0  \n",
       "1    0.133355  0.273627  0.027265  0.000000   0.0  \n",
       "2    0.000000  0.000000  0.000000  0.265349   0.0  \n",
       "3    0.000000  0.502687  0.000000  0.000000   0.0  \n",
       "4    0.000000  0.249126  0.000000  0.020367   0.0  \n",
       "..        ...       ...       ...       ...   ...  \n",
       "409  0.000000  0.015707  0.000000  0.000000   0.0  \n",
       "410  0.000000  0.000000  0.000000  0.000000   0.0  \n",
       "411  0.097417  0.020793  0.000000  0.172312   0.0  \n",
       "412  0.000000  0.000000  0.000000  0.890431   0.0  \n",
       "413  0.000000  0.000000  0.000000  0.556341   0.0  \n",
       "\n",
       "[414 rows x 1500 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test logistic model fit using extracted dae features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(features_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_model.predict(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8599033816425121"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode test data and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(test_df[feature_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df = pd.DataFrame(X_test, columns=feature_cols)\n",
    "noisy_X_test = add_noise(X_test_df, p=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.6315\n",
      "Epoch 2/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.3718\n",
      "Epoch 3/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.2782\n",
      "Epoch 4/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.2229\n",
      "Epoch 5/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.1783\n",
      "Epoch 6/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1546\n",
      "Epoch 7/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.1338\n",
      "Epoch 8/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.1154\n",
      "Epoch 9/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.1029\n",
      "Epoch 10/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0919\n",
      "Epoch 11/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0788\n",
      "Epoch 12/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0697\n",
      "Epoch 13/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0599\n",
      "Epoch 14/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0547\n",
      "Epoch 15/2001\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0488\n",
      "Epoch 16/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0452\n",
      "Epoch 17/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0419\n",
      "Epoch 18/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0374\n",
      "Epoch 19/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0329\n",
      "Epoch 20/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0302\n",
      "Epoch 21/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0294\n",
      "Epoch 22/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0258\n",
      "Epoch 23/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0252\n",
      "Epoch 24/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0227\n",
      "Epoch 25/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0207\n",
      "Epoch 26/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0213\n",
      "Epoch 27/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0212\n",
      "Epoch 28/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0191\n",
      "Epoch 29/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0173\n",
      "Epoch 30/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0173\n",
      "Epoch 31/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0147\n",
      "Epoch 32/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0156\n",
      "Epoch 33/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0153\n",
      "Epoch 34/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0136\n",
      "Epoch 35/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0138\n",
      "Epoch 36/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0129\n",
      "Epoch 37/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0134\n",
      "Epoch 38/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0129\n",
      "Epoch 39/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0117\n",
      "Epoch 40/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0106\n",
      "Epoch 41/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0104\n",
      "Epoch 42/2001\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0091\n",
      "Epoch 43/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0080\n",
      "Epoch 44/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0067\n",
      "Epoch 45/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0067\n",
      "Epoch 46/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0060\n",
      "Epoch 47/2001\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0058\n",
      "Epoch 48/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0057\n",
      "Epoch 49/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0057\n",
      "Epoch 50/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0064\n",
      "Epoch 51/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0060\n",
      "Epoch 52/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0052\n",
      "Epoch 53/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0049\n",
      "Epoch 54/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0046\n",
      "Epoch 55/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0048\n",
      "Epoch 56/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0048\n",
      "Epoch 57/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0043\n",
      "Epoch 58/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0050\n",
      "Epoch 59/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0050\n",
      "Epoch 60/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0045\n",
      "Epoch 61/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0039\n",
      "Epoch 62/2001\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0043\n",
      "Epoch 63/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0038\n",
      "Epoch 64/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0040\n",
      "Epoch 65/2001\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0035\n",
      "Epoch 66/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0039\n",
      "Epoch 67/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0038\n",
      "Epoch 68/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0037\n",
      "Epoch 69/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0033\n",
      "Epoch 70/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0032\n",
      "Epoch 71/2001\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 0.0035\n",
      "Epoch 72/2001\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0035\n",
      "Epoch 73/2001\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0037\n",
      "Epoch 74/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0035\n",
      "Epoch 75/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0034\n",
      "Epoch 76/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0033\n",
      "Epoch 77/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0038\n",
      "Epoch 78/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0036\n",
      "Epoch 79/2001\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.0034\n",
      "Epoch 80/2001\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.0035\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(noisy_X_test, X_test_df, epochs=2001,\n",
    "                    callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "test_features_df = extract_features(model, X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_model.predict(test_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = test_df.copy()\n",
    "sub_df['target'] = y_pred\n",
    "sub_df = sub_df[['id', 'target']]\n",
    "sub_df.to_csv(os.path.join(proj_dir, 'data', 'submissions', 'dae_sn30_model_submission_1.csv'), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitting the above gives me a score of 0.75333 ... even worse than the lr model alone.  Obviously need to tune at least the swap_noise probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
